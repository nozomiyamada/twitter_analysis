{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import json, html, re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.test.utils import datapath\n",
    "from datetime import timedelta, datetime\n",
    "from pythainlp import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "STOPWORDS = thai_stopwords()\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "font_path = '/Users/Nozomi/Library/Fonts/THSarabunNew.ttf'\n",
    "font_label = fm.FontProperties(fname=font_path,weight='bold',size=25)\n",
    "\n",
    "def clean(text, hashtag=None):\n",
    "    if hashtag:\n",
    "        text = text.replace(hashtag, '')\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'http.+?(?:\\\\s|$)', '', text) # URL link\n",
    "    text = re.sub(r'[“”„]', '\"', text) # convert double quotations into \"\n",
    "    text = re.sub(r'[‘’′′′′`]', \"'\", text) # convert single quotations into '\n",
    "    text = re.sub(r'[ \\u00a0\\xa0\\u3000\\u2002-\\u200a\\t\\n#]+', ' ', text) # shrink whitespaces e.g. good  boy -> good boy\n",
    "    text = re.sub(r'[\\r\\u200b\\ufeff]+', '', text) # remove non-breaking space\n",
    "    text = re.sub(r'ํา','ำ', text) # am\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize(text, hashtag=None):\n",
    "    tokens = word_tokenize(clean(text, hashtag), keep_whitespace='False')\n",
    "    tokens = [token for token in tokens if token not in STOPWORDS and re.match(r'[ก-๙][ก-๙\\\\.\\\\-]+$', token)]\n",
    "    return tokens\n",
    "\n",
    "def get_hour(row, string=True):\n",
    "    date = datetime(row.date.year, row.date.month, row.date.day, int(row.time[:2]))\n",
    "    if string:\n",
    "        return str(date)[:-3] # '2021-04-09 22:00'\n",
    "    else:\n",
    "        return date # datetime.datetime(2021, 4, 9, 22, 0)\n",
    "\n",
    "    \n",
    "def timerange(df):\n",
    "    oldest = get_hour(df.iloc[len(df)-1], False)\n",
    "    latest = get_hour(df.iloc[0], False)\n",
    "    range_list = [str(oldest)]\n",
    "    while range_list[-1] != str(latest):\n",
    "        oldest = oldest + timedelta(hours=1)\n",
    "        range_list.append(str(oldest))\n",
    "    return pd.DataFrame(range_list, columns=['date'])\n",
    "\n",
    "def timerange_day(df):\n",
    "    oldest = df.iloc[len(df)-1].date\n",
    "    latest = df.iloc[0].date\n",
    "    range_list = [str(oldest).split()[0]]\n",
    "    while range_list[-1] != str(latest).split()[0]:\n",
    "        oldest = oldest + timedelta(days=1)\n",
    "        range_list.append(str(oldest).split()[0])\n",
    "    return pd.DataFrame(range_list, columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#16ตุลาไปแยกปทุมวัน.csv\n",
      "#16ตุลาไปแยกปทุมวัน.json\n",
      "#25พฤศจิกาไปSCB.csv\n",
      "#25พฤศจิกาไปSCB.json\n",
      "#saveวันเฉลิม.csv\n",
      "#saveวันเฉลิม.json\n",
      "#ม็อบ25พฤศจิกาทวงคืนสมบัติชาติ.csv\n",
      "#ม็อบ25พฤศจิกาทวงคืนสมบัติชาติ.json\n",
      "#ม็อบ2ธันวา.csv\n",
      "#ม็อบ2ธันวา.json\n",
      "#ธรรมศาสตร์และการชุมนุม.csv\n",
      "#ธรรมศาสตร์และการชุมนุม.json\n",
      "#กูสั่งให้มึงอยู่ใต้รัฐธรรมนูญ.csv\n",
      "#เกียมอุดมไม่ก้มหัวให้เผด็จการ.csv\n",
      "#กูสั่งให้มึงอยู่ใต้รัฐธรรมนูญ.json\n",
      "#เกียมอุดมไม่ก้มหัวให้เผด็จการ.json\n",
      "badstudent_tweet.csv\n",
      "badstudent_tweet.json\n",
      "badstudent_tweet_raw.csv\n",
      "freeyouth_tweet.csv\n",
      "freeyouth_tweet.json\n",
      "freeyouth_tweet_raw.csv\n",
      "tanawatofficial_tweet.csv\n",
      "tanawatofficial_tweet.json\n",
      "tanawatofficial_tweet_raw.csv\n",
      "thammasatUFTD_tweet.csv\n",
      "thammasatUFTD_tweet.json\n",
      "thammasatUFTD_tweet_raw.csv\n",
      "ประเทศกูมี.csv\n",
      "ประเทศกูมี.json\n",
      "ประเทศกูมี_tokenized.json\n",
      "ถ้าการเมืองดี.json\n",
      "ให้มันจบที่รุ่นเรา.json\n"
     ]
    }
   ],
   "source": [
    "!ls tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tweets/ประเทศกูมี.json'\n",
    "filename = 'tweets/ถ้าการเมืองดี.json'\n",
    "num = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(filename, lines=True).drop_duplicates('id')\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "### define duration ###\n",
    "df = df[(df.date >= '2020-1-1') & (df.date <= '2020-12-31')]\n",
    "\n",
    "### tokenize ###\n",
    "df['tokens'] = df.tweet.apply(lambda x: tokenize(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(filename.replace('.json','_tokenized.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# frequent hashtag & cooccurent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(filename.replace('.json','_tokenized.json'))\n",
    "\n",
    "\n",
    "##### hashtags #####\n",
    "hashtag_counter = Counter()\n",
    "for lst in tqdm(df.hashtags):\n",
    "    hashtag_counter.update(lst)\n",
    "    \n",
    "hashtag2word = {x[0]:Counter() for x in hashtag_counter.most_common(num)}\n",
    "\n",
    "# count frequent words\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    for hashtag in row['hashtags']:\n",
    "        if hashtag in hashtag2word:\n",
    "            hashtag2word[hashtag].update(row['tokens'])\n",
    "    \n",
    "print(\"|hashtag|count|frequent words|oldest|popular|\\n|:-:|:-:|:-:|:-:|:-:|\")\n",
    "for tag, c, in hashtag_counter.most_common(num):\n",
    "    tempdf = df[df.hashtags.apply(lambda x: tag in x)].sort_values(['created_at'])\n",
    "    \n",
    "    ## frequent words\n",
    "    freqs = \" \".join([f\"{w}({c})\" for w,c in hashtag2word[tag].most_common(10)])\n",
    "    \n",
    "    ## oldest\n",
    "    oldest = tempdf.iloc[0].tweet\n",
    "    oldest_name = tempdf.iloc[0].username\n",
    "    oldest_link = tempdf.iloc[0].link\n",
    "    oldest_date = str(tempdf.iloc[0]['created_at']).split('+')[0]\n",
    "    \n",
    "    ## popular\n",
    "    tempdf = tempdf.sort_values([\"retweets_count\"], ascending=False)\n",
    "    popular = tempdf.iloc[0].tweet\n",
    "    popular_name = tempdf.iloc[0].username\n",
    "    popular_link = tempdf.iloc[0].link\n",
    "    popular_date = str(tempdf.iloc[0]['created_at']).split('+')[0]\n",
    "    retweet_count = tempdf.iloc[0].retweets_count\n",
    "    \n",
    "    print(f\"|{tag}|{c}|{freqs}|[{oldest_date}]({oldest_link})<br>[@{oldest_name}](https://twitter.com/{oldest_name})<br><br>{oldest.replace('|||','')}|[{popular_date}]({popular_link})<br>[@{popular_name}](https://twitter.com/{popular_name})<br>{retweet_count} retweets<br><br>{popular.replace('|||','')}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['date','time','username','tweet','hashtags','link','quote_url','urls','photos','thumbnail','mentions','reply_to','replies_count','retweets_count','likes_count']\n",
    "\n",
    "df[column].to_csv(filename.replace('.json','.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hashtag timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot time series ###\n",
    "\n",
    "start_m = 1\n",
    "xs = list(range(start_m, 13))\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "for tag, count in tqdm(hashtag_counter.most_common(11)[1:]): # exclude original hashtag\n",
    "    counts = []  \n",
    "    for m in range(start_m, 13):\n",
    "        count = 0\n",
    "        thismonth_df = df[df.date.dt.month == m]\n",
    "        for i, row in thismonth_df.iterrows():\n",
    "            if tag in row['hashtags']:\n",
    "                count += 1\n",
    "        counts.append(count)\n",
    "\n",
    "    plt.plot(xs, counts, label=tag)\n",
    "\n",
    "plt.xticks(xs, size=15)\n",
    "plt.yticks(size=15)\n",
    "plt.legend(prop=font_label)\n",
    "plt.xlabel('month', size=25)\n",
    "plt.ylabel('count of co-hashtag', size=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "##### TRAIN #####\n",
    "\n",
    "num_topic = 5\n",
    "\n",
    "corpus_dictionary = Dictionary(df.tokens)\n",
    "corpus = []\n",
    "for tokens_list in tqdm(df.tokens):\n",
    "    corpus.append(corpus_dictionary.doc2bow(tokens_list))\n",
    "lda = LdaModel(corpus, num_topics=num_topic, id2word=corpus_dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk.\n",
    "temp_file = datapath(filename.replace('tweets/','').replace('.json',''))\n",
    "lda.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "#lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### print result of keywords ###\n",
    "print('|rank|topic 1||topic 2||topic 3||topic 4||topic 5||\\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|')\n",
    "for i in range(10):\n",
    "    print(f'|{i+1}', end='')\n",
    "    for j in range(5):\n",
    "        word, score = lda.show_topic(j)[i]\n",
    "        print(f\"|{word}|{score:.3f}\",end=\"\")\n",
    "    print('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prediction - argmax ###\n",
    "\n",
    "n = len(df)\n",
    "\n",
    "result_matrix = np.zeros((n, num_topic))\n",
    "for row, dist in enumerate(lda.get_document_topics(corpus[:n])):\n",
    "    for tpl in dist:\n",
    "        col = tpl[0]\n",
    "        result_matrix[row,col] = tpl[1]\n",
    "        \n",
    "result_df = pd.DataFrame(result_matrix)\n",
    "result_df['topic'] = result_df.apply(lambda row: np.argmax(row), axis=1)\n",
    "result_df['tweet'] = df.tweet.reset_index(drop=True)\n",
    "result_df['date'] = df.apply(lambda row: get_hour(row), axis=1).reset_index(drop=True)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### print prediction ###\n",
    "\n",
    "print('|tweet|topic|prob of 1|2|3|4|5|\\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|')\n",
    "for i, row in result_df[['tweet','topic',0,1,2,3,4]].sample(10).iterrows():\n",
    "    print(f'|{row.tweet}|{row.topic+1}|{row[0]:.3f}|{row[1]:.3f}|{row[2]:.3f}|{row[3]:.3f}|{row[4]:.3f}|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot topic time series ###\n",
    "\n",
    "tempdf = timerange_day(df)\n",
    "\n",
    "for t in range(5):\n",
    "    tempdf[t+1] = tempdf.date.apply(lambda x: sum((result_df.date <= x) & (result_df.topic==t)))\n",
    "    \n",
    "tempdf.plot(x='date', figsize=(20,15))\n",
    "plt.legend(prop=font_label)\n",
    "plt.ylabel('cumulative count', size=20)\n",
    "plt.xlabel('datetime', size=20)\n",
    "plt.xticks(rotation=40, size=15)\n",
    "plt.yticks(size=15)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
